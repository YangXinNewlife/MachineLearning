基础数据，返回数据机和对应的label标签
dataSet = [['yes'],['yes'],['no'],['no'],['no']]
labels  露出水面   脚蹼

香农公式
每一组feature下的某个分类下，香农熵的信息期望
将当前实例的标签存储，即每一行数据的最后一个数据代表的是标签
为所有可能的分类创建字典，如果当前的键值不存在，则扩展字典并将当前键值加入字典。每个键值都记录了当前类别出现的次数。
对于label标签的占比，求出label标签的香农熵
使用所有类标签的发生频率计算类别出现的概率
计算香农熵，以 2 为底求对数



splitDataSet(通过遍历dataSet数据集，求出index对应的colnum列的值为value的行)
    就是依据index列进行分类，如果index列的数据等于 value的时候，就要将 index 划分到我们创建的新的数据集中
Args:
    dataSet 数据集                 待划分的数据集
    index 表示每一行的index列        划分数据集的特征
    value 表示index列对应的value值   需要返回的特征的值。
Returns:
    index列为value的数据集【该数据集需要排除index列】


请百度查询一下： extend和append的区别
list.append(object) 向列表中添加一个对象object
list.extend(sequence) 把一个序列seq的内容添加到列表中
1、使用append的时候，是将new_media看作一个对象，整体打包添加到music_media对象中。
2、使用extend的时候，是将new_media看作一个序列，将这个序列和music_media序列合并，并放在其后面。
result = []
result.extend([1,2,3])
print result
result.append([4,5,6])
print result
result.extend([7,8,9])
print result
结果：
[1, 2, 3]
[1, 2, 3, [4, 5, 6]]
[1, 2, 3, [4, 5, 6], 7, 8, 9]


chooseBestFeatureToSplit(选择最好的特征)

Args:
    dataSet 数据集
Returns:
    bestFeature 最优的特征列



 # -----------选择最优特征的第一种方式 end------------------------------------

        # # -----------选择最优特征的第二种方式 start------------------------------------
        # # 计算初始香农熵
        # base_entropy = calcShannonEnt(dataSet)
        # best_info_gain = 0
        # best_feature = -1
        # # 遍历每一个特征
        # for i in range(len(dataSet[0]) - 1):
        #     # 对当前特征进行统计
        #     feature_count = Counter([data[i] for data in dataSet])
        #     # 计算分割后的香农熵
        #     new_entropy = sum(feature[1] / float(len(dataSet)) * calcShannonEnt(splitDataSet(dataSet, i, feature[0])) \
        #                    for feature in feature_count.items())
        #     # 更新值
        #     info_gain = base_entropy - new_entropy
        #     print('No. {0} feature info gain is {1:.3f}'.format(i, info_gain))
        #     if info_gain > best_info_gain:
        #         best_info_gain = info_gain
        #         best_feature = i
        # return best_feature
        # # -----------选择最优特征的第二种方式 end------------------------------------

"""majorityCnt(选择出现次数最多的一个结果)

        Args:
            classList label列的集合
        Returns:
            bestFeature 最优的特征列
        """




"""classify(给输入的节点，进行分类)

        Args:
            inputTree  决策树模型
            featLabels Feature标签对应的名称
            testVec    测试输入的数据
        Returns:
            classLabel 分类的结果值，需要映射label才能知道名称
        """


        # print myDat, labels

        # 计算label分类标签的香农熵
        # calcShannonEnt(myDat)

        # # 求第0列 为 1/0的列的数据集【排除第0列】
        # print '1---', splitDataSet(myDat, 0, 1)
        # print '0---', splitDataSet(myDat, 0, 0)

        # # 计算最好的信息增益的列
        # print chooseBestFeatureToSplit(myDat)



     """
        Desc:
            预测隐形眼镜的测试代码
        Args:
            none
        Returns:
            none
        """



 """
         Desc:
            递归获得决策树的高度
        Args:
            tree
        Returns:
            树高
        """


         # -----------majorityCnt的第一种方式 end------------------------------------

        # # -----------majorityCnt的第二种方式 start------------------------------------
        # major_label = Counter(classList).most_common(1)[0]
        # return major_label
        # # -----------majorityCnt的第二种方式 end------------------------------------


